{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlwaysEden/Machine-Learning-HW/blob/main/HW8_kNN_and_Random_Forest_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FGrw6kiveUAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n",
        "1.   Implement and apply kNN on MNIST with k=1, 5, 10. Apply kNN on raw images, and 2, 7 dimensional eigenspaces, respectively. Show the accuracy scores for each run (you'd run the algorithm 9 times).\n",
        "\n",
        "2.   Use and run the Random Forest algorithm for MNIST classification (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Show the accuracy scores.\n",
        "\n",
        "If it takes too much time, reduce the number of samples for training/testing (based on random selection)."
      ],
      "metadata": {
        "id": "XtZTSiDRAp2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import urllib.request\n",
        "import gzip\n",
        "import pickle"
      ],
      "metadata": {
        "id": "J5cik59AByis"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(dataset):\n",
        "\n",
        "    if not os.path.isfile(dataset):\n",
        "        origin = (\n",
        "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
        "        )\n",
        "        print('Downloading data from %s' % origin)\n",
        "        urllib.request.urlretrieve(origin, dataset)\n",
        "\n",
        "    print('Loading data...')\n",
        "\n",
        "    with gzip.open(dataset, 'rb') as f:\n",
        "        try:\n",
        "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "        except:\n",
        "            train_set, valid_set, test_set = pickle.load(f)\n",
        "\n",
        "    all_data_x = np.concatenate([train_set[0], valid_set[0], test_set[0]])\n",
        "    all_data_y = np.concatenate([train_set[1], valid_set[1], test_set[1]])\n",
        "\n",
        "    print('... data has been loaded!')\n",
        "    return all_data_x, all_data_y\n",
        "\n",
        "def PCA(X, n_components):\n",
        "    # Step 1: Center the data\n",
        "    X_centered = X - np.mean(X, axis=0)\n",
        "\n",
        "    # Step 2: Compute the covariance matrix\n",
        "    covariance_matrix = np.cov(X_centered, rowvar=False)\n",
        "\n",
        "    # Step 3: Eigen decomposition\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
        "\n",
        "    # Step 4: Sort eigenvectors by eigenvalues in descending order\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "    # Step 5: Select the top 'n_components' eigenvectors\n",
        "    W = sorted_eigenvectors[:, :n_components]\n",
        "\n",
        "    # Step 6: Project the data onto the new feature space\n",
        "    X_pca = X_centered.dot(W)\n",
        "\n",
        "    return X_pca\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "  return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "def splitData(data_x, data_y):\n",
        "  train_size = int(0.7 * data_x.shape[0])\n",
        "  val_size = int(0.15 * data_x.shape[0])\n",
        "  test_size = data_x.shape[0] - train_size - val_size\n",
        "\n",
        "  #Random\n",
        "  indices = np.arange(data_x.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  data_x = data_x[indices]\n",
        "  data_y = data_y[indices]\n",
        "\n",
        "  train_x = data_x[:train_size]\n",
        "  train_y = data_y[:train_size]\n",
        "  val_x = data_x[train_size:train_size+val_size]\n",
        "  val_y = data_y[train_size:train_size+val_size]\n",
        "  test_x = data_x[train_size+val_size:]\n",
        "  test_y = data_y[train_size+val_size:]\n",
        "\n",
        "  print(\"... data has been splited\")\n",
        "  return train_x, train_y, val_x, val_y, test_x, test_y\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  correct = np.sum(y_true == y_pred)\n",
        "  return correct / len(y_true)\n",
        "\n",
        "def kNN(target_x, target_y, k):\n",
        "  arr = np.zeros((target_x.shape[0],target_x.shape[0]))\n",
        "  arr[np.diag_indices_from(arr)] = 10000 # (n,n) = 10000 for distinguishing\n",
        "\n",
        "  neighbors = np.zeros((target_x.shape[0],k))\n",
        "\n",
        "  #Look for k neighbors.\n",
        "  for i in range(target_x.shape[0]):\n",
        "    for j in range(i+1, target_x.shape[0]):\n",
        "      arr[i][j] = euclidean_distance(target_x[i], target_x[j])\n",
        "      arr[j][i] = arr[i][j] #For reduction of duplicate computation\n",
        "    for x in range(k):\n",
        "      neighbors[i][x] = np.argmin(arr[i])\n",
        "      arr[i][int(neighbors[i][x])] = 10000\n",
        "\n",
        "  #Classification Vote\n",
        "  pred_y = []\n",
        "  for i in range(target_x.shape[0]):\n",
        "    new = []\n",
        "    for j in range(k):\n",
        "      new.append(target_y[int(neighbors[i][j])])\n",
        "    tmp_arr, count = np.unique(new, return_counts=True)\n",
        "    pred_y.append(tmp_arr[np.argmax(count)])\n",
        "\n",
        "  return pred_y"
      ],
      "metadata": {
        "id": "ba74DTNDCua4"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_x, data_y= load_data('mnist.pkl.gz')\n",
        "train_x, train_y, val_x, val_y, test_x, test_y = splitData(data_x, data_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP73Z_U6Cw5c",
        "outputId": "7b7a404e-a99b-4249-c453-4ac0c25564a4"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "... data has been loaded!\n",
            "... data has been splited\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#kNN\n",
        "\n",
        "''' If you want to reduce the number of data\n",
        "indices = np.arange(data_x.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "data_x = data_x[indices]\n",
        "data_y = data_y[indices]\n",
        "data_x = data_x[:1000]\n",
        "data_y = data_y[:1000]\n",
        "'''\n",
        "# k = 1, raw_images\n",
        "pred_y = kNN(data_x, data_y, 1)\n",
        "print(f\"k=1 & raw_image => acc = {accuracy(data_y, pred_y)}\")\n",
        "\n",
        "# k = 1, 2D\n",
        "data_x_pca = PCA(data_x, 2)\n",
        "pred_y = kNN(data_x_pca, data_y, 1)\n",
        "print(f\"k=1 & 2D => acc = {accuracy(data_y, pred_y)}\")\n",
        "\n",
        "# k = 1, 7D\n",
        "data_x_pca = PCA(data_x, 7)\n",
        "pred_y = kNN(data_x_pca, data_y, 1)\n",
        "print(f\"k=1 & 7D => acc = {accuracy(data_y, pred_y)}\")\n",
        "\n",
        "# k = 5, raw_images\n",
        "pred_y = kNN(data_x, data_y, 5)\n",
        "print(f\"k=5 & raw_image => acc = {accuracy(data_y, pred_y)}\")\n",
        "\n",
        "# k = 5, 2D\n",
        "data_x_pca = PCA(data_x, 2)\n",
        "\n",
        "pred_y = kNN(data_x_pca, data_y, 1)\n",
        "print(f\"k=5 & 2D => acc = {accuracy(data_y, pred_y)}\")\n",
        "\n",
        "# k = 5, 7D\n",
        "data_x_pca = PCA(data_x, 7)\n",
        "\n",
        "pred_y = kNN(data_x_pca, data_y, 1)\n",
        "print(f\"k=5 & 7D => acc = {accuracy(data_y, pred_y)}\")\n",
        "\n",
        "# k = 10, raw_images\n",
        "pred_y = kNN(data_x, data_y, 10)\n",
        "print(f\"k=10 & raw_image => acc = {accuracy(data_y, pred_y)}\")\n",
        "\n",
        "# k = 10, 2D\n",
        "data_x_pca = PCA(data_x, 2)\n",
        "pred_y = kNN(data_x_pca, data_y, 10)\n",
        "print(f\"k=10 & 2D => acc = {accuracy(data_y, pred_y)}\")\n",
        "\n",
        "# k = 10, 7D\n",
        "data_x_pca = PCA(data_x, 7)\n",
        "pred_y = kNN(data_x_pca, data_y, 10)\n",
        "print(f\"k=10 & 7D => acc = {accuracy(data_y, pred_y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYrdDi2rOvPo",
        "outputId": "d0b1b147-9321-43c7-b19c-4fa97247c3e3"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k=1 & raw_image => acc = 0.899\n",
            "k=1 & 2D => acc = 0.399\n",
            "k=1 & 7D => acc = 0.77\n",
            "k=5 & raw_image => acc = 0.87\n",
            "k=5 & 2D => acc = 0.399\n",
            "k=5 & 7D => acc = 0.77\n",
            "k=10 & raw_image => acc = 0.854\n",
            "k=10 & 2D => acc = 0.437\n",
            "k=10 & 7D => acc = 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "''' If you want to reduce the number of train, val, and test data.\n",
        "n = 1000 #the number of data\n",
        "\n",
        "indices = np.arange(train_x.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_x = train_x[indices]\n",
        "train_y = train_y[indices]\n",
        "train_x = train_x[:n]\n",
        "train_y = train_y[:n]\n",
        "\n",
        "indices = np.arange(val_x.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "val_x = val_x[indices]\n",
        "val_y = val_y[indices]\n",
        "val_x = val_x[:n]\n",
        "val_y = val_y[:n]\n",
        "\n",
        "indices = np.arange(test_x.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "test_x = test_x[indices]\n",
        "test_y = test_y[indices]\n",
        "test_x = test_x[:n]\n",
        "test_y = test_y[:n]\n",
        "'''\n",
        "\n",
        "#raw_image\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42) # n_estimators는 트리 개수, random_state는 랜덤 시드\n",
        "rf_classifier.fit(train_x, train_y)\n",
        "y_pred_val = rf_classifier.predict(val_x)\n",
        "y_pred_test = rf_classifier.predict(test_x)\n",
        "accuracy_val = accuracy(val_y, y_pred_val)\n",
        "accuracy_test = accuracy(test_y, y_pred_test)\n",
        "oob_score = rf_classifier.oob_score_\n",
        "print(f\"OOB Score(raw_image): {oob_score}\")\n",
        "print(f\"val_acc(raw_image): {accuracy_val}\")\n",
        "print(f\"test_acc(raw_image): {accuracy_test}\")\n",
        "\n",
        "\n",
        "#2D\n",
        "rf_classifier = RandomForestClassifier(n_estimators=300, oob_score=True, random_state=42) # n_estimators는 트리 개수, random_state는 랜덤 시드\n",
        "train_x_pca = PCA(train_x, 2)\n",
        "val_x_pca = PCA(val_x, 2)\n",
        "test_x_pca = PCA(test_x, 2)\n",
        "rf_classifier.fit(train_x_pca, train_y)\n",
        "y_pred_val = rf_classifier.predict(val_x_pca)\n",
        "y_pred_test = rf_classifier.predict(test_x_pca)\n",
        "accuracy_val = accuracy(val_y, y_pred_val)\n",
        "accuracy_test = accuracy(test_y, y_pred_test)\n",
        "oob_score = rf_classifier.oob_score_\n",
        "print(f\"OOB Score(2D): {oob_score}\")\n",
        "print(f\"val_acc(2D): {accuracy_val}\")\n",
        "print(f\"test_acc(2D): {accuracy_test}\")\n",
        "\n",
        "#7D\n",
        "rf_classifier = RandomForestClassifier(n_estimators=300, oob_score=True,random_state=42) # n_estimators는 트리 개수, random_state는 랜덤 시드\n",
        "train_x_pca = PCA(train_x, 7)\n",
        "val_x_pca = PCA(val_x, 7)\n",
        "test_x_pca = PCA(test_x, 7)\n",
        "rf_classifier.fit(train_x_pca, train_y)\n",
        "y_pred_val = rf_classifier.predict(val_x_pca)\n",
        "y_pred_test = rf_classifier.predict(test_x_pca)\n",
        "accuracy_val = accuracy(val_y, y_pred_val)\n",
        "accuracy_test = accuracy(test_y, y_pred_test)\n",
        "oob_score = rf_classifier.oob_score_\n",
        "print(f\"OOB Score(7D): {oob_score}\")\n",
        "print(f\"val_acc(7D): {accuracy_val}\")\n",
        "print(f\"test_acc(7D): {accuracy_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVzeMBY1i2Gj",
        "outputId": "124c418b-a151-4844-927b-6b7f3e78ab73"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score(raw_image): 0.89\n",
            "val_acc(raw_image): 0.902\n",
            "test_acc(raw_image): 0.906\n",
            "OOB Score(2D): 0.419\n",
            "val_acc(2D): 0.364\n",
            "test_acc(2D): 0.116\n",
            "OOB Score(7D): 0.801\n",
            "val_acc(7D): 0.524\n",
            "test_acc(7D): 0.192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT prompts\n",
        "\n",
        "Now solve the same task with ChatGPT with the following prompts:\n",
        "\n",
        "\n",
        "1.   Can you implement and apply kNN on MNIST (from scratch without using any specialized library) with k=1, 5, 10? Can you show the accuracies of the predictions? Ignore the computational cost of this code.\n",
        "2.   Can you use the same provided knn algorithm and apply it to the MNIST data with reduced dimensions to 2 and 7, with k=1, 5, 10? Please provide the accuracies.\n",
        "3.   Can you provide a code for the random forest algorithm applied to the MNIST dataset? Please use specialized libraries like sklearn. Please show the accuracies scores.\n",
        "\n",
        "\n",
        "**Paste the answer provided and run the code below:**"
      ],
      "metadata": {
        "id": "6idIZhGPB1wZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ChatGPT Prompts 1\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def load_data(dataset):\n",
        "\n",
        "    if not os.path.isfile(dataset):\n",
        "        origin = (\n",
        "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
        "        )\n",
        "        print('Downloading data from %s' % origin)\n",
        "        urllib.request.urlretrieve(origin, dataset)\n",
        "\n",
        "    print('Loading data...')\n",
        "\n",
        "    with gzip.open(dataset, 'rb') as f:\n",
        "        try:\n",
        "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "        except:\n",
        "            train_set, valid_set, test_set = pickle.load(f)\n",
        "\n",
        "    print('... data has been loaded!')\n",
        "    return train_set, valid_set, test_set\n",
        "\n",
        "# k-NN function to make predictions\n",
        "def knn_predict(X_train, y_train, X_test, k):\n",
        "    predictions = []\n",
        "    for test_point in X_test:\n",
        "        # Compute distances to all training points\n",
        "        distances = np.sqrt(np.sum((X_train - test_point) ** 2, axis=1))\n",
        "\n",
        "        # Find the k nearest neighbors and their labels\n",
        "        k_indices = np.argsort(distances)[:k]\n",
        "        k_nearest_labels = y_train[k_indices]\n",
        "\n",
        "        # Predict the most common label\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)[0][0]\n",
        "        predictions.append(most_common)\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Function to evaluate the k-NN algorithm for different k values\n",
        "def evaluate_knn(X_train, y_train, X_test, y_test, k_values):\n",
        "    accuracies = {}\n",
        "    for k in k_values:\n",
        "        y_pred = knn_predict(X_train, y_train, X_test, k)\n",
        "        accuracy = np.mean(y_pred == y_test) * 100  # Calculate accuracy as a percentage\n",
        "        accuracies[k] = accuracy\n",
        "        print(f\"Accuracy for k={k}: {accuracy:.2f}%\")\n",
        "    return accuracies\n",
        "\n",
        "# Example usage with training and test data\n",
        "# X_train, y_train, X_test, y_test should be defined beforehand\n",
        "train_set, val_set, test_set = load_data('mnist.pkl.gz')\n",
        "X_train, y_train = train_set\n",
        "X_test, y_test = test_set\n",
        "\n",
        "k_values = [1, 5, 10]\n",
        "accuracies = evaluate_knn(X_train, y_train, X_test, y_test, k_values)\n"
      ],
      "metadata": {
        "id": "yNdFVuAzBz-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975c711c-9322-48f1-edf4-0d123ac53143"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "... data has been loaded!\n",
            "Accuracy for k=1: 87.90%\n",
            "Accuracy for k=5: 87.80%\n",
            "Accuracy for k=10: 87.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ChatGPT Prompts 2\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Function to reduce dimensions with PCA\n",
        "def reduce_dimensions(X, n_components):\n",
        "    # Center the data\n",
        "    X_centered = X - np.mean(X, axis=0)\n",
        "    # Compute covariance matrix\n",
        "    covariance_matrix = np.cov(X_centered, rowvar=False)\n",
        "    # Compute eigenvalues and eigenvectors\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
        "    # Select the top 'n_components' eigenvectors\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    top_eigenvectors = eigenvectors[:, sorted_indices[:n_components]]\n",
        "    # Project data onto top eigenvectors\n",
        "    return np.dot(X_centered, top_eigenvectors)\n",
        "\n",
        "# k-NN function to make predictions\n",
        "def knn_predict(X_train, y_train, X_test, k):\n",
        "    predictions = []\n",
        "    for test_point in X_test:\n",
        "        # Compute distances to all training points\n",
        "        distances = np.sqrt(np.sum((X_train - test_point) ** 2, axis=1))\n",
        "        # Find the k nearest neighbors and their labels\n",
        "        k_indices = np.argsort(distances)[:k]\n",
        "        k_nearest_labels = y_train[k_indices]\n",
        "        # Predict the most common label\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)[0][0]\n",
        "        predictions.append(most_common)\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Function to evaluate the k-NN algorithm for different k values\n",
        "def evaluate_knn(X_train, y_train, X_test, y_test, k_values):\n",
        "    accuracies = {}\n",
        "    for k in k_values:\n",
        "        y_pred = knn_predict(X_train, y_train, X_test, k)\n",
        "        accuracy = np.mean(y_pred == y_test) * 100  # Calculate accuracy as a percentage\n",
        "        accuracies[k] = accuracy\n",
        "        print(f\"Accuracy for k={k}: {accuracy:.2f}%\")\n",
        "    return accuracies\n",
        "\n",
        "# Example usage with dimensionally reduced data\n",
        "# X_train, y_train, X_test, y_test should be defined beforehand\n",
        "train_set, val_set, test_set = load_data('mnist.pkl.gz')\n",
        "X_train, y_train = train_set\n",
        "X_test, y_test = test_set\n",
        "# Define k values to evaluate\n",
        "k_values = [1, 5, 10]\n",
        "\n",
        "# Reduce MNIST dataset dimensions to 2 and evaluate k-NN\n",
        "X_train_reduced_2 = reduce_dimensions(X_train, 2)\n",
        "X_test_reduced_2 = reduce_dimensions(X_test, 2)\n",
        "print(\"Results for 2 dimensions:\")\n",
        "accuracies_2 = evaluate_knn(X_train_reduced_2, y_train, X_test_reduced_2, y_test, k_values)\n",
        "\n",
        "# Reduce MNIST dataset dimensions to 7 and evaluate k-NN\n",
        "X_train_reduced_7 = reduce_dimensions(X_train, 7)\n",
        "X_test_reduced_7 = reduce_dimensions(X_test, 7)\n",
        "print(\"\\nResults for 7 dimensions:\")\n",
        "accuracies_7 = evaluate_knn(X_train_reduced_7, y_train, X_test_reduced_7, y_test, k_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7rf-jOQyXxn",
        "outputId": "de56243f-2b1e-4d6b-bc5d-6324b42588c6"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "... data has been loaded!\n",
            "Results for 2 dimensions:\n",
            "Accuracy for k=1: 3.60%\n",
            "Accuracy for k=5: 3.60%\n",
            "Accuracy for k=10: 4.40%\n",
            "\n",
            "Results for 7 dimensions:\n",
            "Accuracy for k=1: 2.30%\n",
            "Accuracy for k=5: 1.50%\n",
            "Accuracy for k=10: 1.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ChatGPT prompts 3\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml(\"mnist_784\", version=1)\n",
        "X = mnist.data\n",
        "y = mnist.target.astype(int)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Random Forest on MNIST dataset: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paAm6Szzyxsf",
        "outputId": "97b10e3b-15b9-4482-946e-a55e007e0dae"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Random Forest on MNIST dataset: 89.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report\n",
        "\n",
        "\n",
        "1.   What are some differences you notice between your code and ChatGPT's? (Name at least 3)\n",
        "2.   Ask ChatGPT to compare kNN and Random Forest. Paste the answer here. Do you agree with ChatGPT's statement? Why yes or why not?\n",
        "3.   In yours and ChatGPTs implementation, which algorithm had the highest accuracy?\n",
        "4.   Conclude with some thoughts and things you learned from this homework."
      ],
      "metadata": {
        "id": "TZPU5Iz9C2IS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer\n",
        "1.\n",
        "- Majority voting is different.\n",
        "- Scores are measured differently\n",
        "- Different ways to find neighbors\n",
        "2. I agree with most GPT's idea. However, I have some thought about a few idea. First, ChatGPT said that kNN has a very fast training time. It could be possible in terms of training time but kNN algorithm take a lot of time in terms of whole algorithm because of looking for k neighbors. Second, ChatGPT said that kNN struggles with high-dimensional data. Although ChatGPT said it, kNN show me high performance using raw images.\n",
        "3. My Random Forest has the highest accuracy. Surely the difference between my RF and ChatGPT's RF is 1%.\n",
        "4. This assignment was my first introduction to models. It was my first experience with the process of training and predicting a model, and I learned the importance of data by considering whether the model was overfitted when measuring accuracy.\n",
        "\n",
        "\n",
        "\n",
        "#ChatGPT's Answer about Q2\n",
        "k-Nearest Neighbors (kNN) and Random Forest (RF) are both widely used machine learning algorithms, but they differ fundamentally in their approach, performance characteristics, and optimal use cases. Here’s a detailed comparison:\n",
        "\n",
        "### 1. **Algorithm Type and Approach**\n",
        "   - **kNN**:\n",
        "     - kNN is a **non-parametric**, **instance-based** (or lazy) learning algorithm. It doesn't explicitly learn a model; instead, it stores the training data and makes predictions by comparing a new instance to all training instances, selecting the \\( k \\) nearest neighbors based on a chosen distance metric (often Euclidean).\n",
        "     - Prediction is typically based on the majority class among the neighbors (classification) or the average value (regression).\n",
        "   - **Random Forest**:\n",
        "     - Random Forest is a **parametric**, **ensemble** algorithm that builds multiple decision trees during training and aggregates their predictions. Each tree is trained on a subset of the data (with replacement, called bootstrapping), and a random subset of features is considered for splitting at each node.\n",
        "     - Final predictions are based on the majority vote (classification) or average (regression) of all trees, helping to reduce variance and avoid overfitting.\n",
        "\n",
        "### 2. **Training and Prediction Speed**\n",
        "   - **kNN**:\n",
        "     - **Training Time**: Very fast since kNN only stores the data without building a model.\n",
        "     - **Prediction Time**: Relatively slow, especially on large datasets, since kNN computes the distance from a test point to every point in the training set.\n",
        "   - **Random Forest**:\n",
        "     - **Training Time**: Slower compared to kNN as it requires constructing many decision trees, but modern implementations (e.g., in `sklearn`) can be efficient.\n",
        "     - **Prediction Time**: Faster than kNN for large datasets because predictions only require passing the instance through a fixed set of decision trees.\n",
        "\n",
        "### 3. **Memory Requirements**\n",
        "   - **kNN**: Requires storage of the entire dataset, which can lead to high memory usage, especially with large datasets.\n",
        "   - **Random Forest**: Requires storage of the model (trees), but generally more memory-efficient as it doesn’t need to store all instances for prediction.\n",
        "\n",
        "### 4. **Accuracy and Performance**\n",
        "   - **kNN**: Performs well on smaller datasets with low-dimensional, well-separated classes. However, it struggles with high-dimensional or sparse data due to the **curse of dimensionality**.\n",
        "   - **Random Forest**: Often achieves higher accuracy, particularly on complex or large datasets with high-dimensional features. Its ensemble approach helps it generalize better and reduce overfitting.\n",
        "\n",
        "### 5. **Interpretability**\n",
        "   - **kNN**: Simple to understand; the prediction is based directly on the nearest neighbors.\n",
        "   - **Random Forest**: Harder to interpret due to the complex ensemble of trees, but **feature importance** scores can give insights into which features are most influential.\n",
        "\n",
        "### 6. **Handling High-Dimensional Data**\n",
        "   - **kNN**: Struggles with high-dimensional data, as it becomes challenging to find meaningful neighbors in many dimensions.\n",
        "   - **Random Forest**: Handles high-dimensional data better because it randomly selects subsets of features for each tree, thus reducing dimensionality implicitly.\n",
        "\n",
        "### 7. **Applications**\n",
        "   - **kNN**: Often used in recommender systems, basic image and text classification, and cases where interpretability and simplicity are priorities.\n",
        "   - **Random Forest**: Commonly used in structured/tabular data applications, credit scoring, medical diagnosis, and cases where predictive performance is prioritized over interpretability.\n",
        "\n",
        "### 8. **Effectiveness on MNIST Dataset**\n",
        "   - **kNN**: May perform adequately but can be inefficient on the full MNIST dataset due to high-dimensionality (784 features) and large size.\n",
        "   - **Random Forest**: Tends to perform better on MNIST, achieving higher accuracy due to its robustness against high dimensionality and ability to capture complex patterns in the data.\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Criterion                  | kNN                               | Random Forest                        |\n",
        "|----------------------------|-----------------------------------|--------------------------------------|\n",
        "| **Algorithm Type**         | Non-parametric, instance-based   | Parametric, ensemble (tree-based)    |\n",
        "| **Training Time**          | Fast                             | Moderate                            |\n",
        "| **Prediction Time**        | Slow                             | Fast                                |\n",
        "| **Memory Requirements**    | High (stores dataset)            | Moderate (stores trees)             |\n",
        "| **Curse of Dimensionality**| Affected                         | Less affected                       |\n",
        "| **Accuracy**               | Decent for low-dimensional data  | Generally higher on complex data    |\n",
        "| **Interpretability**       | High                             | Moderate                            |\n",
        "| **Use Case**               | Small, low-dimensional datasets  | Large, high-dimensional datasets    |\n",
        "| **Effectiveness on MNIST** | Moderate                         | High                                |\n",
        "\n",
        "### Conclusion\n",
        "For the MNIST dataset:\n",
        "- **Random Forest** is generally more effective**, offering higher accuracy and faster prediction speeds after training, especially suitable given MNIST’s high-dimensional nature.\n",
        "- **kNN** can work on MNIST but is typically less efficient due to its slower prediction time and sensitivity to high-dimensionality, which may reduce accuracy without dimension reduction.\n",
        "\n",
        "In most practical cases, Random Forest outperforms kNN on MNIST in terms of both accuracy and scalability."
      ],
      "metadata": {
        "id": "XJh_TG5mz4ve"
      }
    }
  ]
}